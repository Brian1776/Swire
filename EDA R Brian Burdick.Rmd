---
title: "EDA Swire Cola"
author: "Brian Burdick"
date: "2023-02-17"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Business Problem

  Business to Business sales of Swire Coca-Cola represent 10% of their total sales.  Swire wants to maximize earning potential of contracts with new        restaurants while at the same time ensuring that they win the business by creating a greater offering to the restaurant then their competitors.  The      offering is based on the projected profitability of the new restaurant.  When the restaurants thrive, it creates a loyal and valuable customer and when   it doesn’t thrive it represents a significant loss to Swire Coca-Cola. Simply put Swire wants to know the longevity and sales of these restaurants so     they can quantify the maximum offerings they can give and still make a profit or walk away from the business altogether. 

Analytical Objective

  The analytical approach that we may use includes creating models with best-in-class techniques such as xgboost, knn, support vector machines, and         logistic regression to identify the best performing models on our target variables which includes the total sales and whether a restaurant will go out    of business. Our outcome variable is total profit GROSS_PROFIT_DEAD_NET.


Exploration: Questions, Plots, and interpretation


```{r , echo=FALSE}
data <- read.csv(file.choose(), stringsAsFactors = FALSE) #Read in data Sales Data
customer <- read.csv(file.choose(), stringsAsFactors = FALSE) #Read in data Customer Data

```


```{r , echo=FALSE}
library(dplyr)
library(ggplot2)

```



Customer Questions
•	How many customers does each sales office serve as a % of total?
 This table shows that Draper Utah office accounts for 0.1124 or 11.24% of all customers in this data set.This also shows the proportion
 of all other sales offices.

```{r , echo=FALSE}

cust <- table(customer$SALES_OFFICE_DESCRIPTION)
prop.table(cust) 
```

•	How many customers does each delivery plant have?
This is the proportion of customers served by each delivery plant.  With Draper Utah having 11.24% of all customers i find it interesting that
the Draper delivery plant serves roughly the same percentage of customers at 11.24%
```{r , echo=FALSE}
del <- table(customer$DELIVERY_PLANT_DESCRIPTION)
prop.table(del) 
```

•	What is the distribution of Activity Cluster
54% of all distribution is from Eating&Drinking, 9.28% is Entertainment and Recreation, 2.46% is grocery stores.   I found the educational percentage to be lower then i would have expected at 3.09%.  This table shows the proportion of activity cluster.  

```{r , echo=FALSE}
custact <- table(customer$CUSTOMER_ACTIVITY_CLUSTER_DESCRIPTION)
prop.table(custact) 

```

•	What is the distribution of Trade Cluster
Full Service and Quick Service Restaurants are 16.51% and 25.48% which represents the majority of the cluster.  This percentage makes sense as to why we would want to figure out the sales and longevity of restaurants in particular.  Perhaps when we train our model we will focus our efforts on building a model to predict Full Service and Quick Service Restaurants.  During the presentation they had mentioned that the soda machines cost about $15,000 a piece and in many cases are lost if the company goes under.  Seems to me like a lot of the financial risk in the discounts they offer is whether or not they provided one of these machines?

```{r , echo=FALSE}
chan <- table(customer$CUSTOMER_TRADE_CHANNEL_DESCRIPTION)
prop.table(chan)
```

•	What is the distribution of sub trade

```{r , echo=FALSE}
sub <- table(customer$CUSTOMER_SUB_TRADE_CHANNEL_DESCRIPTION)
prop.table(sub)
```

•	What is the distribution of trade channel?
This shows the proportion of the type.  Out of the QSR type i would have expected Pizza (4.19%) or Hamburger (4.33%) to be the largest proportion of customers but Mexican is at 4.46%.

```{r , echo=FALSE}
tchan <- table(customer$CUSTOMER_SUB_TRADE_CHANNEL_DESCRIPTION)
prop.table(tchan)
```


Customer Data Questions

•	What is the average physical volume purchased?
The average physical volume is 36.22 but the minimum is -65832.  Are customers able to return purchased items?

```{r , echo=FALSE}

#Changed to numeric variables from null
data$PHYSICAL_VOLUME <- as.numeric(data$PHYSICAL_VOLUME)
data$DISCOUNT <- as.numeric(data$DISCOUNT)
data$NSI <- as.numeric(data$DEAD_NET)
data$INVOICE_PRICE <- as.numeric(data$INVOICE_PRICE)
data$DEAD_NET <- as.numeric(data$DEAD_NET)
data$GROSS_PROFIT_DEAD_NET <- as.numeric(data$GROSS_PROFIT_DEAD_NET)
data$COGS <- as.numeric(data$COGS)

summary(data$PHYSICAL_VOLUME)

```



•	What is the average discount offered?
The mean is 1003.8 and median is 90.0  which means the graph is left skewed because the mean is to the right of the median.  

```{r , echo=FALSE}

summary(data$DISCOUNT)


```


•	What is the profit to COGS per order as a scatter plot?
To see a better representation of this data i dropped COGS exceeding 15000 and profit of 10000 to see the plot better.  Their does to appear to be a linear relationship between the profit and cost of goods sold.  This graph suggests that the more that is sold the more linear the relationship is between profit and cogs.  As COGS increase profit also appears to be linear between 0 and 2500 

```{r , echo=FALSE}

subdata <-subset(data, data$COGS < 15000 & data$COGS >= 0 & data$GROSS_PROFIT_DEAD_NET < 10000 & data$GROSS_PROFIT_DEAD_NET >= 0)

plot(subdata$COGS, subdata$GROSS_PROFIT_DEAD_NET,
     main="Scatter plot between Profit and COGS",
     xlab="COGS",
     ylab="Profit")
```

•	What does a scatter plot look like between discount and gross profit?
Upon doing the scatter plot on the original data set their were outliers that were obscuring the pattern between the x and y variables.  I performed a subset of the data on discount at 30,000 and profit at 10,000.  This generally shows that the larger the discount the higher the profit.  
```{r , echo=FALSE}

subdata2 <- subset(data, data$DISCOUNT < 30000 & data$DISCOUNT >= 0 & data$GROSS_PROFIT_DEAD_NET < 10000 & data$GROSS_PROFIT_DEAD_NET > 0)

plot(subdata2$DISCOUNT, subdata2$GROSS_PROFIT_DEAD_NET,
     main="Scatter plot between Profit and Discount",
     xlab="Discount",
     ylab="Profit")
```
•	What does a scatter plot look like between volume and discount?
This shows a linear relationship between units purchased and the discount offered where the larger the volume the more of a discount is received. However their is an interesting pattern where a group of orders are showing deeper discounts given their volume purchased of individual products.

```{r , echo=FALSE}

subdata3 <- subset(data, data$PHYSICAL_VOLUME < 500 & data$PHYSICAL_VOLUME > 0 &  data$DISCOUNT < 25000 & data$DISCOUNT >= 0)

plot(subdata3$PHYSICAL_VOLUME, subdata3$DISCOUNT,
     main="Scatter plot between Volume and Discount",
     xlab="Volume",
     ylab="Discount")
```





```{r , echo=FALSE}




#Bev_Cat color

subdata3 <- subset(data, data$PHYSICAL_VOLUME < 500 & data$PHYSICAL_VOLUME > 0 &  data$DISCOUNT < 25000 & data$DISCOUNT >= 0)

ggplot(subdata3, aes(PHYSICAL_VOLUME, DISCOUNT, color = BEV_CAT_DESC)) +
  geom_point() +
  labs(title = "Scatter plot between Volume and Discount Color BEV_CAT_DESC",
       x = "Volume",
       y = "Discount") +
  theme_minimal()


#Pack size color
subdata3a <- subset(data, data$PHYSICAL_VOLUME < 500 & data$PHYSICAL_VOLUME > 0 &  data$DISCOUNT < 25000 & data$DISCOUNT >= 0 & BEV_CAT_DESC == "CORE SPARKLING" & CALORIE_CAT_DESC == "REGULAR CALORIE" & PACK_TYPE_DESC == "Bag-In-Box")

ggplot(subdata3a, aes(PHYSICAL_VOLUME, DISCOUNT, color = PACK_SIZE_SALES_UNIT_DESCRIPTION)) +
  geom_point() +
  labs(title = "Scatter plot between Volume and Discount Color PACK_SIZE_SALES_UNIT_DESCRIPTION",
       x = "Volume",
       y = "Discount") +
  theme_minimal()



#Product Sold color
subdata3b <- subset(data, data$PHYSICAL_VOLUME < 500 & data$PHYSICAL_VOLUME > 0 &  data$DISCOUNT < 25000 & data$DISCOUNT >= 0 & BEV_CAT_DESC == "CORE SPARKLING" & CALORIE_CAT_DESC == "REGULAR CALORIE" & PACK_TYPE_DESC == "Bag-In-Box" & PACK_SIZE_SALES_UNIT_DESCRIPTION == "5 GALLON 1-Ls")

ggplot(subdata3b, aes(PHYSICAL_VOLUME, DISCOUNT, color = PRODUCT_SOLD_BLINDED)) +
  geom_point() +
  labs(title = "Scatter plot between Volume and Discount",
       x = "Volume",
       y = "Discount") +
  theme_minimal()



```

•	What is number of transactions vs discount.
I would have expected to see a linear relationship between these two variables but am not seeing the pattern.  

```{r , echo=FALSE}


subdata10 <- subset(data, data$DISCOUNT < 50000 & data$DISCOUNT >= 0 & data$NUM_OF_TRANSACTIONS >= 0 & data$NUM_OF_TRANSACTIONS <= 500)


plot(subdata10$NUM_OF_TRANSACTIONS, subdata10$DISCOUNT,
     main="Scatter plot between Discount and Transactions",
     xlab="NUM_OF_TRANSACTIONS",
     ylab="Discount")

```

•	What is the profit/cogs by discount as a scatter plot?
I took the profit and divided it by cogs to get a ratio that i then created a scatter plot on volume to see what the relationship would look like between these.  As volume decreases the ratio of profit to COGS increases.  This suggests that the more volume that is purchased the better of a discount they get.

```{r , echo=FALSE}

data$P_Cogs <- data$GROSS_PROFIT_DEAD_NET/data$COGS

  
subdata4 <- subset(data, data$P_Cogs <= 5 & data$P_Cogs > 0 & data$PHYSICAL_VOLUME > 0 & data$PHYSICAL_VOLUME <= 1500)


plot(subdata4$P_Cogs, subdata4$PHYSICAL_VOLUME,
     main="Scatter plot between Profit to COGS Ratio and Discount to Volume Ratio",
     xlab="Profit to COGS Ratio",
     ylab="Volume")

```


•	What does a scatter plot look like between number of transactions and discount?
Their does not appear to be a linear relationship between number of transactions and discount.  

```{r , echo=FALSE}

subdata32 <- subset(data, data$NUM_OF_TRANSACTIONS < 500 & data$NUM_OF_TRANSACTIONS > 0 &  data$DISCOUNT < 40000 & data$DISCOUNT >= 0)

plot(subdata32$NUM_OF_TRANSACTIONS, subdata32$DISCOUNT,
     main="Scatter plot between Volume and Discount",
     xlab="NUM_OF_TRANSACTIONS",
     ylab="Discount")

```



•	What is the top 10 best selling products in terms of profit?
Best selling of the 10 is M014407280247 with total profit of $17,471,066	

```{r , echo=FALSE}

y <- sum(data$GROSS_PROFIT_DEAD_NET)

x <- aggregate(GROSS_PROFIT_DEAD_NET ~ PRODUCT_SOLD_BLINDED, data=data, FUN=sum)

top_n(x,n=10)

```

•	Who is the top 10 customers in terms of profit?
Our best customer is C0036030908720224 and has made us $2,528,505.70

```{r , echo=FALSE}

x <- aggregate(GROSS_PROFIT_DEAD_NET ~ CUSTOMER_NUMBER_BLINDED, data=data, FUN=sum)

top_n(x,n=10)

```


Results Section

•	As volume decreases the ratio of profit to COGS increases.  This suggests that the more volume that is purchased the better of a discount they get.This seemed like a strong correlation overall.
•	Draper Utah office accounts for 0.1224 or 12.24% of all customers in this data set
•	Data Problems: I've found negative values in discounts, quantity and other fields numeric fields.  
•	Out of the QSR type i would have expected Pizza (4.64%) or Hamburger (4.04%) to be the largest proportion of customers but Mexican is at 6.8%.
•	Full Service and Quick Service Restaurants are 31.68% and 25.36% which represents the majority of the cluster.  This percentage makes sense as to why we would want to figure out the sales and longevity of restaurants in particular.  Perhaps when we train our model we will focus our efforts on building a model to predict Full Service and Quick Service Restaurants.  During the presentation they had mentioned that the soda machines cost about $15,000 a piece and in many cases are lost if the company goes under.  Seems to me like a lot of the financial risk in the discounts they offer is whether or not they provided one of these machines.  How this EDA influenced my analytical approach is if we should use the whole data set to build our model or to just focus on the Restaurant segment.  


Ethical Considerations
If I were Swire I would have had the students sign an NDA or Non Compete agreement. With having the lat and long, discount amount, units, and date of order one could come up with a pretty good idea as to the business name, business model, and where to open up a competing shop.


Results Section

•	As volume decreases the ratio of profit to COGS increases.  This suggests that the more volume that is purchased the better of a discount they get.This seemed like a strong correlation overall.
•	Draper Utah office accounts for 0.1224 or 12.24% of all customers in this data set
•	Data Problems: I've found negative values in discounts, quantity and other fields numeric fields.  
•	Out of the QSR type i would have expected Pizza (4.64%) or Hamburger (4.04%) to be the largest proportion of customers but Mexican is at 6.8%.
•	Full Service and Quick Service Restaurants are 31.68% and 25.36% which represents the majority of the cluster.  This percentage makes sense as to why we would want to figure out the sales and longevity of restaurants in particular.  Perhaps when we train our model we will focus our efforts on building a model to predict Full Service and Quick Service Restaurants.  During the presentation they had mentioned that the soda machines cost about $15,000 a piece and in many cases are lost if the company goes under.  Seems to me like a lot of the financial risk in the discounts they offer is whether or not they provided one of these machines.  How this EDA influenced my analytical approach is if we should use the whole data set to build our model or to just focus on the Restaurant segment.  


Ethical Considerations
If I were Swire I would have had the students sign an NDA or Non Compete agreement. With having the lat and long, discount amount, units, and date of order one could come up with a pretty good idea as to the business name, business model, and where to open up a competing shop.  








#########################
#####################################
##############################################





#Model Building

#Introduction: Business and Analytics Problem

•	Swire Coca-Cola wants to maximize their earnings by ensuring that when deep discounts are given they make financial sense for the bottom line.  They would like models that can accurately predict longevity of the customer and total sales over a three year period both of which could be predicted by many types of models including linear models, xgboost, NN, and support vector machines.  

#Appropriate Models

•	These are the models I thought about implementing 1) Linear Model because both of our output variables are numeric. 2) GLM Model: from my experience these seem to perform a little better than regular linear models.  3) Neural Net: these perform really well regardless of whether the predictor variables are linearly related to the outcome variables.  4) Lasso because it penalizes coefficients down to 0 so the model is more generalizable when predicting.  I decided to perform the linear and glm models due to being able to train them in a timely manner.  I did attempt to do Lasso but after 45 minutes+ of the model training I dropped it and chose not to do the Neural Net for the same reason.  I am going to use a linear model and GLM to predict the longevity and 3-year sales. 

```{r , echo=TRUE}

md <- read.csv(file.choose(), stringsAsFactors = FALSE) #Read in the data to analyze


library(glmnet)
library(caret)
library(tidyr)

```


#Data Preparation and Feature Engineering

•	I performed these steps in Excel (had this been a database I would have done this transformation in SQL). The assignment is to calculate the longevity of the business i.e., 1,2,3 years and three years of sales.  Since we were only given two years’ worth of historic sales, I multiplied the two years by 1.5 to arrive at estimated three years’ worth of sales.  I took the onboard date and max date to arrive on the longevity of the business.  I also took any customers with a negative profit and set it to zero so my models wouldn’t be thrown off by negative values. 

•	On the customer table I performed the following steps to the data 1) Added Field if sales office city was equal to address city, 2) Added Field if sales office was the same city as delivery plant, 3) Standardized zip code to 5 digit, 4) Dropped latitude and longitude, 5) Standardized on boarding date.  Upon the completion of that I combined both tables together.

•	I added additional columns to the data 1) median income for 2019, 2020, 2021 based on county, 2)median income in county as a percentage of national median, 3) county population and 4) county class.  

•	The predictor variables I used were Sales = Delivery,	sales_office = address_city,	CUSTOMER_ACTIVITY_CLUSTER_DESCRIPTION,	CUSTOMER_TRADE_CHANNEL_DESCRIPTION,	CUSTOMER_SUB_TRADE_CHANNEL_DESCRIPTION,	BUSINESS_TYPE_EXTENSION_DESCRIPTION,	CUSTOMER_TRADE_CHANNEL_DESCRIPTION2,	MARKET_DESCRIPTION,	COLD_DRINK_CHANNEL_DESCRIPTION,	Population Code Category,	year 2019,	year 2020,	year 2021,	% of US Median 2019, 	% of US 2020,	% of US Median 2021, and 	County Population as predictors.  My two outcome variables are longevity and 3 year sales.  


#Data Preparation: Variable transformation

•	When Swire Coca-Cola go to sell products to new restaurants/customers they will not have information regarding Package size, package, profit, physical volume, invoice price, discount, cogs, or gross profit.  I decided to get rid of any information that they would not have going into a sales meeting with a restaurant that hasn't opened yet. I wanted to see if a meaningful model could be built based on the data they would have going into the sales meeting. I will perform analysis on both of these output variables independent of one another. I split the data into 80% train and 20% test so that I could train the model on 80% and test it on the remaining 20%.  I converted categorical variables represented by columns 1 through 10 to factor variables.  I dropped sales from the longevity table and longevity from the sales tables because I didn't want them to be used as predictors for each other so created two data sets to be used in my modeling.  

```{r , echo=TRUE}

#Converted categorical variables into factor variables.  
md[c(1,2,3,4,5,6,7,8,9,10)]<-data.frame(lapply(md[c(1,2,3,4,5,6,7,8,9,10)],factor)) 

#Changed data type from character to numeric
md$year.2019 <- as.numeric(md$year.2019)
md$year.2020 <- as.numeric(md$year.2020)
md$year.2021 <- as.numeric(md$year.2021)
md$X..of.US.Median.2019 <- as.numeric(md$X..of.US.Median.2019)
md$X..of.US.2020 <- as.numeric(md$X..of.US.2020)
md$X..of.US.Median.2021 <- as.numeric(md$X..of.US.Median.2021)
md$County.Population <- as.numeric(md$County.Population)

#this is longevity data frame that drops the 3 year sales and others as noted above.
mdL <- md[,-18]

#this is sales that drops longevity and others as noted above.
mds <- md[,-19]

#Longevity train and test set
set.seed(1234)
nRows <- nrow(mdL) # gives you the number of rows
train.size.L <- floor(nRows*0.8)
train.index.L <- sample(1:nRows, train.size.L, replace=F)
train.L <- mdL[train.index.L,]  
test.L <- mdL[-train.index.L,]

#Sales train and test set
set.seed(123)
nRows <- nrow(mds) # gives you the number of rows
train.size.s <- floor(nRows*0.8)
train.index.s <- sample(1:nRows, train.size.s, replace=F)
train.s <- mds[train.index.s,]  
test.s <- mds[-train.index.s,]
```

##Linear Model
This is a linear model for predicting customer longevity and using all predictor variables to train and test the model.  After training and testing the model my performance metric of RMSE came back at 8.707738 for Longevity. I will talk about coefficients; p values and what model performs better in model interpretation. Model Train Time: 8.860184 seconds


```{r , echo=TRUE}

start <- Sys.time() # Start tracking time
modellml<- lm(Customer.Longevity~.,train.L) #Train model
end <- Sys.time() # End tracking time
predictL <- predict(modellml, data = test.L) #Test model 
RMSE(predictL, test.L$Customer.Longevity, na.rm = FALSE) #Model Metric
print(end-start) # Print run time
```

This is a linear model for predicting customer sales and using all predictor variables to train the model. After training and testing the model my performance metric of RMSE came back at RMSE is 39668.14 for Sales. I will talk about coefficients; p values and what model performs better in model interpretation. Model Train Time: 0.9503791 secs
 
```{r , echo=TRUE}
start.ls <- Sys.time() # Start tracking time
modellms<- lm(X3.Year.Period.Profit~.,train.s)  #Train model
end.ls <- Sys.time() # End tracking time
predicts <- predict(modellms, data = test.s) #Test model 
RMSE(predicts, test.s$X3.Year.Period.Profit, na.rm = FALSE) #Model Metric
print(end.ls-start.ls) # Print run time
```

##GLM Model

This is a GLM model for predicting customer longevity and using all predictor variables to train the model.After training and testing the model my performance metric of RMSE came back at RMSE is 8.713989 for longevity. I will talk about coefficients; p values and what model performs better in model interpretation. Model Train Time: 1.852106 secs

```{r , echo=TRUE}
start.lg <- Sys.time() # Start tracking time
model.L <- glm(Customer.Longevity ~., data = train.L, family = gaussian) #Train model
end.lg <- Sys.time() # End tracking time
model.Lt <- predict(model.L, data = test.L) #Test model 
RMSE(model.Lt, test.L$Customer.Longevity, na.rm = FALSE) #Model Metric
print(end.lg-start.lg) # Print run time
```

This is a GLM model for predicting customer sales and using all predictor variables to train the model. After training and testing the model my performance metric of RMSE came back at RMSE is 39676.36 for sales. I will talk about coefficients; p values and what model performs better in model interpretation. Model Train Time: 1.897175 secs

```{r , echo=TRUE}
start.sg <- Sys.time() # Start tracking time
model.s <- glm(X3.Year.Period.Profit~., data = train.s, family = gaussian) #Train model
end.sg <- Sys.time() # End tracking time
model.st <- predict(model.s, data = test.L) #Test model 
RMSE(model.st, test.s$X3.Year.Period.Profit, na.rm = FALSE) #Model Metric
print(end.sg-start.sg) # Print run time
```

#Evaluation: Strengths and weaknesses of each model and what metric I used to select the best performing model based on the model characterisitics

•	Generally speaking, the strengths and weaknesses of these models are as follows.  Linear models are popular because they are 1) Easy to implement and interpret by viewing impacts of the coefficient on the dependent variable, 2) can handle large data sets pretty efficiently, 3) ability to use p values to see if the impact of the variable is having a statistically significant impact on the outcome variable.  Some of the weaknesses of using a linear model include 1) assumes it is a linear relationship between predictor and outcome variables, 2) can require additional transformations at times, 3) assumes dependent variables have a constant variance across the levels of the independent variables which may or may not be true.  A general linearized model has several strengths and weaknesses.  Strengths 1) can model a range of different data types, 2) easy to interpret coefficients on the output variable, 3) AIC can be used to compare against other models.  Weaknesses include 1) can become complex when multiple predictors are included, 2) data is assumed to follow a specific distribution, 3) can become overfit if there are to many predictors in the model.  

•	I trained the model using 80% of the data and tested it on the remaining 20% to validate the model.  I used Root Mean Squared Error as the performance metric in comparing the models to one another.  I chose to go with the LM model because it performed better than the GLM for both the longevity and 3 year sales.  Comparing RMSE of longevity LM (8.707738) vs GLM (8.713989) and sales LM (39668.14) vs GLM (39676.36) shows that the linear model performs better than the GLM.  Comparing run time of LM longevity GLM longevity LM sales 

#Model Interpretation

Rather than go through and interpret all of the coefficients and p values I decided to just not the things that stood out to me for my chosen LM model. 
•	County.Population had a positive correlation for sales but a negative correlation for longevity both of which were statistically significant with p values under .05.
•	Transportation for sub trade channel on both sales and longevity were positive and statistically significant with a p value under .05.
•	longevity for median income for 2019 was negatively correlated but in 2020 it was positively correlated. 

#Results and Business Validation

No, my results are not sufficient to solve the business problem.  My RMSE for the linear model (8.82 Longevity and $58,241.66 for sales)  and GLM were high on both the longevity and 3 year sales relative to the amounts.  Most of the data provided i.e. sales information was data that was known by Swire after the deal has already been made with the new customer which could not be used to inform the kind of discounts could be offered.  Although the models that were used were good models my predictor variables were not sufficient to create a useful model.  I believe a large part of it was that most of the predictor values were factor and not numeric which makes it difficult for linear models to predict.  I would recommend additional data to be gathered in order to increase the accuracy of the model such as customer information both about the owner and about the physical facility.  Although it would be challenging to get information about the person or company that is starting the company it would be very beneficial to gather this information for example do they own other businesses, how many years of business experience do they have, who is their target market for their goods etc.  Additional information for restaurants in particular would be drive thru, number of tables and chairs, seating capacity, number of parking spots, app ordering, etc.  Adding these predictors to the model would most likely increase the accuracy of the model.  After going through this exercise, I would have liked to see the outcome variable be what factors determine the discount given and are those the right discounts to offer those customers.  